---
title: "DSAA Technical Exam"
author: "Aditya_Trivedi"
email: "trivedimchemist@gmail.com"
date: "06/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 0. load the libraries
```{r}
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(gtsummary))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(caTools))
suppressPackageStartupMessages(library(cvms))
suppressPackageStartupMessages(library(caret))

```

# 1. Reading the two datasets and replace the blanks with "na"

```{r}
data1<-read.csv('mimic_ii_data.csv', na.strings = c("","na"))
data1=data1%>%arrange(str_length(data1$patient_id),patient_id) # since patient prefix is same so lets do length of string and I will do ascending sort before merging to data2
data2<-read.csv('mimic_ii_demographics.csv', na.strings = c("","na"))
```

### 1.1 data output and overview
```{r}
head(data1,2)  # output to see the data1; showing only 2 rows
head(data2,2)  # output to see the data2
dim(data1) # data1 dimensions to see the total variable count and rows
dim(data2) # data2 dimensions
```

### 1.2 join the datasets, primary key is pat_id

```{r}
merged<-merge(data1,data2,by = "patient_id")
merged=merged%>%arrange(str_length(merged$patient_id),patient_id)
head(merged,2)
```
### 1.3 missing values overview
```{r}
missing_vals_merged_dataset<-sapply(merged,function(y)sum(is.na(y)))
missing_values_count<-data.frame(missing_vals_merged_dataset) # count the number of missing values in data1+data2 
missing_values_count
```


### 1.4 clean the variables for the merged dataset
```{r}
data3=select(merged,
             patient_id,
             age,
             gender_num,
             weight_first,
             bmi,
             sapsi_first,
             sofa_first,
             aline_flg,
             stroke_flg,
             resp_flg,
             afib_flg, # I think there is a typo in the instructions so using *afib* instead of *aline* as it is mentioned twice
             day_28_flg
             )
head(data3,2)
```
# 2. Data Statistics 

### 2.1 frequncy output showing count per variable
```{r}
frq_data= select(data3,gender_num,aline_flg,stroke_flg,resp_flg,afib_flg,day_28_flg)
all<-apply(frq_data,2,table)
all
```
### 2.2 cross tab 
```{r}
cross_tab=data3%>%tbl_cross(row = day_28_flg,col = aline_flg)
cross_tab
```
### 2.3 Stats for numeric values

```{r}
num_data<-select(merged,age,weight_first,bmi,sapsi_first,sofa_first)
summary(num_data)
```

```{r}

```
### The above shows that the median age of individuals in the dataset if 54 and median weight of 80kg. The variable BMI has most missing values.

### 2.4 Graphical representation

```{r}
# distribution of weight_first with age
ggplot(num_data, aes(x=age,y=weight_first))+
  geom_point(size=0.5)+ 
  labs(title = 'Distribution of first weight with age')+ #chart title
  ylab("weight_first (kg)")+
  theme(plot.title = element_text(hjust = 0.5)) #centering the chart title
```

```{r}

```
### The distribution of weight with age shows that the weight of most patients part of this dataset is between 50-100kg. Over age the distribution stays fairly consistent with the trend

```{r}
# distribution of bmi with age
ggplot(num_data, aes(x=age,y=bmi))+
  geom_point(size=0.5)+ 
  labs(title = 'Distribution of BMI with age')+ #chart title
  ylab("Patient BMI")+
  theme(plot.title = element_text(hjust = 0.5)) #centering the chart title
```

```{r}

```
### In the above graph it shows that the sample of patients have a BMI of 25 and as age increases the BMI also stays consistent in this sample size.

```{r}
# distribution of sapsi_first with age
ggplot(num_data, aes(x=sapsi_first,y=bmi))+
  geom_point(size=0.5)+ 
  labs(title = 'Distribution of SAPS I (Simplified Acute Physiology Score) score with bmi')+ #chart title
  ylab("Patient BMI")+
  xlab("SAPS_first")+
  theme(plot.title = element_text(hjust = 0.5)) #centering the chart title
```

```{r}

```

### For this sample, the above graph depicts that the reported SAPS score for majority of patients is between 10-20 and with a BMI less than 50

```{r}
# distribution of sapsi_first with age
ggplot(num_data, aes(x=sofa_first,y=bmi))+
  geom_point(size=0.5)+ 
  labs(title = 'Distribution of SOFA I (Sequenctial Organ Failure Assessment Score) with bmi')+ #chart title
  ylab("Patient BMI")+
  xlab("SOFA_first")+
  theme(plot.title = element_text(hjust = 0.5))+ #centering the chart title
  theme(plot.title = element_text(size = rel(0.9))) # reduce title size
```

```{r}

```

### The SOFA score from the above graph shows that individuals with BMI less than 50 show SOFA score ranging from 3 to 10. There are missing values that were not considered and hence it affects the overall data quality and reduces the sample size.

# 3.0 Dealing with missing data

### 3.1 Doing the data cleaning
```{r}

f_data<-data3%>%drop_na(gender_num) # drop the NA from gender_num
f_data<-data3[rowSums(is.na(data3))>=2,] # drop more than 2 NA values now
dim(f_data) # to see how many rows remain after cleaning the merged dataset
```
### 3.2 Overview of remaining missing values
```{r}
missing_vals_remaining<-sapply(f_data,function(y)sum(is.na(y)))
missing_values_remaining_count<-data.frame(missing_vals_remaining) # count the number of missing values after cleaning
missing_values_remaining_count
```
### 3.3 Data imputation using median for missing "NA" values

```{r}
imupte_numeric_values<- function(data_vector,median){
  for (i in 1:ncol(data_vector)){
    data_vector[,i][is.na(data_vector[,i])]<-median(data_vector[,i],na.rm = TRUE)
  }
  return(data_vector)
}
  
data4<-imupte_numeric_values(f_data,median) # use the function for imputation
dim(data4) #check the dimensions for final dataset for ML task
```
#### 3.3.1 Checking if all missing values replaced 
```{r}
missing_vals_final<-sapply(data4,function(y)sum(is.na(y)))
missing_values_remaining_final<-data.frame(missing_vals_final) # count the number of missing values after cleaning
missing_values_remaining_final
```
```{r}
# Since no missing values remaining from above output so all of them converted to respective column median values
```

# 4.0 Train and Test split

### 4.1 Data shuffling and preparation
```{r}
set.seed(34) #setting seed to have reproducible splits every time it is run
d<-sample(nrow(data4)) #shuffle the dataset
data4<-data4[d,] #then check whether it shows all columns as well
head(data4,2)
split=sample.split(data4,SplitRatio=0.7) #70% split to training rest to test
```

### 4.1.1 Training set
```{r}
train_set<-subset(data4,split==TRUE) #taking 70% of data
dim(train_set)
head(train_set,2)
```

### 4.1.2 Test set
```{r}
test_set<-subset(data4,split==FALSE) # taking 30% of remaining data
dim(test_set)
```

# 5.0 Model fitting to logistic regression
```{r}
Model1<-glm(day_28_flg~ aline_flg,family = binomial,data = train_set) # only aline_flg
Model2<-glm(day_28_flg~
             age+
             gender_num+
             weight_first+
             bmi+
             sapsi_first+
             sofa_first+
             aline_flg+
             stroke_flg+
             resp_flg+
             afib_flg,
             family = binomial,data = train_set) #all variables
summary(Model1)$coef
summary(Model2)$coef
```
### The above results show that none of the variables are statistically significant and do not have strong association with the probability of the patient dying within 28 days.


### 5.1 Fitting on the test 

```{r}
M1_pred<-predict(Model1,newdata = test_set,type = "response")
M2_pred<-predict(Model2,newdata = test_set,type = "response")
```
# 6.0 Evaluation and Confusion matrix 
```{r}
anova(Model1,Model2,test = "Chisq") #anova to compare both models
```
### The p-value for Model 2 is below the significance level and hence the Model 2 is relatively better at predicting the 28 day mortality among patients.

```{r}
confusion_matrix(targets = test_set$day_28_flg, predictions =  as.numeric(M1_pred>0.5),positive = 1)
confusion_matrix(targets = test_set$day_28_flg, predictions =  as.numeric(M2_pred>0.5),positive = 1)
```
### From the confusion matrix results, the F1-score which measures the test's accuracy has a higher value for Model1 and Model 1 has sensitivity of 1 which shows that the model has 100% probability of predicting the 28 day mortality for individuals who did die within 28 days. Detection rate for Model 1 is also higher than that of Model 2 however, it is worthy to test this on a hold out set which was not taken for this project.


# 7.0 Future work
### Since the data size on which the model training occured is extermely small the model results could lead to overfitting on the test set
### Using larger dataset will likely improve the model performance in binary classification. Artificially increase sample size and then training a different model to compare with the baseline logistic regression. Additionally, using a train set, test set and hold out set could be used to further assess model robustness
### For multiclass classificiation, deep neural netwroks such as CNN could be utilized and compared with logistic regression.
### Lastly a ROC curve could be utilized to futher assess model's predictability

